# Copyright (c) 2022-2024, The Isaac Lab Project Developers.
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

from dataclasses import MISSING
from typing import Literal

from isaaclab.utils import configclass
# Add to imports
from typing import Optional

# Add new configuration class
@configclass
class ResidualRLCfg:
    """Configuration for Residual RL with PD control"""
    
    enabled: bool = False
    """Whether to use residual RL mode"""
    
    residual_scale: float = 0.3
    """Scale factor for RL residual actions (0-1)"""
    
    pd_ratio: float = 0.7
    """Ratio of PD control vs RL (0-1)"""
    
    pd_kp: Optional[list[float]] = None
    """Proportional gains for PD controller"""
    
    pd_kd: Optional[list[float]] = None
    """Derivative gains for PD controller"""
    
    curriculum_schedule: bool = True
    """Whether to use curriculum learning for residual scale"""
    
    initial_pd_ratio: float = 0.9
    """Initial PD ratio for curriculum learning"""
    
    final_pd_ratio: float = 0.5
    """Final PD ratio after curriculum"""
    
    curriculum_steps: int = 1000
    """Number of iterations for curriculum"""

@configclass
class CoRlOffPolicyCfg:
    """Configuration for the Off-Policy networks."""

    class_name: str = MISSING
    """The policy class name. Default is SAC."""

    actor_hidden_dims: list[int] = MISSING
    """The hidden dimensions of the actor network."""

    critic_hidden_dims: list[int] = MISSING
    """The hidden dimensions of the critic network."""


####################################################################################
####################################################################################


@configclass
class CoRlPpoActorCriticCfg:
    """Configuration for the PPO actor-critic networks."""

    class_name: str = "ActorCritic"
    """The policy class name. Default is ActorCritic. ActorCriticRecurrent is for RNNs."""

    init_noise_std: float = MISSING
    """The initial noise standard deviation for the policy."""

    actor_hidden_dims: list[int] = MISSING
    """The hidden dimensions of the actor network."""

    critic_hidden_dims: list[int] = MISSING
    """The hidden dimensions of the critic network."""

    activation: str = MISSING
    """The activation function for the actor and critic networks."""


@configclass
class CoRlPpoAlgorithmCfg:
    """Configuration for the PPO algorithm."""

    class_name: str = "PPO"
    """The algorithm class name. Default is PPO."""

    value_loss_coef: float = MISSING
    """The coefficient for the value loss."""

    use_clipped_value_loss: bool = MISSING
    """Whether to use clipped value loss."""

    clip_param: float = MISSING
    """The clipping parameter for the policy."""

    entropy_coef: float = MISSING
    """The coefficient for the entropy loss."""

    num_learning_epochs: int = MISSING
    """The number of learning epochs per update."""

    num_mini_batches: int = MISSING
    """The number of mini-batches per update."""

    learning_rate: float = MISSING
    """The learning rate for the policy."""

    schedule: str = MISSING
    """The learning rate schedule."""

    gamma: float = MISSING
    """The discount factor."""

    lam: float = MISSING
    """The lambda parameter for Generalized Advantage Estimation (GAE)."""

    desired_kl: float = MISSING
    """The desired KL divergence."""

    max_grad_norm: float = MISSING
    """The maximum gradient norm."""


@configclass
class CoRlSrmPpoAlgorithmCfg:
    """Configuration for the PPO algorithm."""

    class_name: str = "SRMPPO"
    """The algorithm class name. Default is PPO."""

    value_loss_coef: float = MISSING
    """The coefficient for the value loss."""

    use_clipped_value_loss: bool = MISSING
    """Whether to use clipped value loss."""

    clip_param: float = MISSING
    """The clipping parameter for the policy."""

    entropy_coef: float = MISSING
    """The coefficient for the entropy loss."""

    num_learning_epochs: int = MISSING
    """The number of learning epochs per update."""

    num_mini_batches: int = MISSING
    """The number of mini-batches per update."""

    learning_rate: float = MISSING
    """The learning rate for the policy."""

    schedule: str = MISSING
    """The learning rate schedule."""

    gamma: float = MISSING
    """The discount factor."""

    lam: float = MISSING
    """The lambda parameter for Generalized Advantage Estimation (GAE)."""

    desired_kl: float = MISSING
    """The desired KL divergence."""

    max_grad_norm: float = MISSING
    """The maximum gradient norm."""

    srm_net: str = MISSING
    """The SRM network type."""

    srm_input_dim: int = MISSING
    """The input dimension for the SRM model."""

    cmd_dim: int = MISSING
    """The command dimension for the SRM model."""

    srm_hidden_dim: int = MISSING
    """The hidden dimension for the SRM model."""

    srm_output_dim: int = MISSING
    """The output dimension for the SRM model."""

    srm_num_layers: int = MISSING
    """The number of layers for the SRM model."""

    srm_r_loss_coef: float = MISSING
    """The coefficient for the SRM reconstruction loss."""

    srm_rc_loss_coef: float = MISSING
    """The coefficient for the SRM reward consistency loss."""

    use_acaps: bool = MISSING
    """Whether to use ACAPS."""

    acaps_lambda_t_coef: float = MISSING
    """The coefficient for the ACAPS temporal loss."""

    acaps_lambda_s_coef: float = MISSING
    """The coefficient for the ACAPS spatial loss."""


@configclass
class CoRlPolicyRunnerCfg:
    
    """Residual RL configuration"""
    residual: ResidualRLCfg = ResidualRLCfg()
    
    """Configuration of the runner for on-policy algorithms."""

    seed: int = 42
    """The seed for the experiment. Default is 42."""

    device: str = "cuda:0"
    """The device for the rl-agent. Default is cuda:0."""

    num_steps_per_env: int = MISSING
    """The number of steps per environment per update."""

    max_iterations: int = MISSING
    """The maximum number of iterations."""

    empirical_normalization: bool = MISSING
    """Whether to use empirical normalization."""

    policy: CoRlPpoActorCriticCfg = MISSING
    """The policy configuration."""

    algorithm: CoRlPpoAlgorithmCfg = MISSING
    """The algorithm configuration."""

    ##
    # Checkpointing parameters
    ##

    save_interval: int = MISSING
    """The number of iterations between saves."""

    experiment_name: str = MISSING
    """The experiment name."""

    experiment_description: str = MISSING
    """The experiment description."""

    run_name: str = ""
    """The run name. Default is empty string.

    The name of the run directory is typically the time-stamp at execution. If the run name is not empty,
    then it is appended to the run directory's name, i.e. the logging directory's name will become
    ``{time-stamp}_{run_name}``.
    """

    ##
    # Logging parameters
    ##

    logger: Literal["tensorboard", "neptune", "wandb"] = "tensorboard"
    """The logger to use. Default is tensorboard."""

    neptune_project: str = "isaaclab"
    """The neptune project name. Default is "isaaclab"."""

    wandb_project: str = "isaaclab"
    """The wandb project name. Default is "isaaclab"."""

    ##
    # Loading parameters
    ##

    resume: bool = False
    """Whether to resume. Default is False."""

    load_run: str = ".*"
    """The run directory to load. Default is ".*" (all).

    If regex expression, the latest (alphabetical order) matching run will be loaded.
    """

    load_checkpoint: str = "model_.*.pt"
    """The checkpoint file to load. Default is ``"model_.*.pt"`` (all).

    If regex expression, the latest (alphabetical order) matching file will be loaded.
    """

    num_policy_stacks: int = 0
    """The number of frames to stack. Default is 0."""

    num_critic_stacks: int = 0
    """The number of frames to stack. Default is 0."""

    use_constraint_rl: bool = False
    """Whether to use constraints as termination."""
########################################################################################################################
########################################################################################################################
########################################################################################################################
