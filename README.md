# DRL_Project: Hybrid PD-PPO Control for Two-Wheeled Legged Robot

## ğŸ“‹ Project Overview

This project implements a novel hybrid control approach for training a two-wheeled legged robot (Flamingo) in Isaac Lab simulation environment. The core innovation combines traditional PD (Proportional-Derivative) control with Proximal Policy Optimization (PPO) reinforcement learning to achieve faster convergence and more stable training compared to pure PPO approaches.

### Key Features
- Implementation of pure PPO baseline for robot locomotion control
- Novel residual reinforcement learning approach combining PD control with PPO
- Comprehensive experimental framework for comparing different control strategies
- Integration with NVIDIA Isaac Lab for physics-based simulation
- Real-time visualization and performance monitoring

## ğŸš€ Novel Approach: PD-Enhanced PPO

### Hypothesis
By integrating PD control as a base controller with PPO learning residual corrections, we can achieve:

1. **Faster Convergence**: "residual RL learns faster than RL alone" [(Johannink et al., 2019)](https://arxiv.org/abs/1812.03201)
2. **Better Stability**: Maintains safe operation during training by leveraging proven control theory
3. **Higher Sample Efficiency**: Leverages existing control knowledge rather than learning from scratch

### Technical Implementation
The approach decomposes the control problem into:
- **Base Controller**: PD control providing fundamental stability
- **Residual Learning**: PPO learns corrective actions on top of PD control
- **Action Composition**: `final_action = Î± * PD_action + (1-Î±) * RL_residual`

## ğŸ§ª Experiment Design

We designed three experimental configurations to validate our hypothesis:

### Experiment 1: Pure PPO Baseline (Purple Line ğŸ’œ)
Standard PPO implementation without any traditional control

**Training Command:**
```bash
python scripts/co_rl/train.py \
  --task Isaac-Velocity-Flat-Flamingo-v1-ppo \
  --algo ppo \
  --num_envs 64 \
  --num_policy_stacks 2 \
  --num_critic_stacks 2 \
  --max_iterations 5000 \
  +learning_rate=0.0003 \
  +num_learning_epochs=10 \
  +num_mini_batches=8 \
  +entropy_coef=0.01
```

**Parameters:**
- Learning rate: 3e-4
- Batch size: 64 environments
- Policy/Critic stacks: 2
- Entropy coefficient: 0.01
- Training epochs per update: 10
- Mini-batches: 8

### Experiment 2: PPO + PD with Moderate Gains (Orange Line ğŸŸ )
Hybrid approach with moderate PD gains for balanced control

**Training Command:**
```bash
python scripts/co_rl/train.py \
    --task Isaac-Velocity-Flat-Flamingo-v1-ppo \
    --algo ppo \
    --num_envs 64 \
    --num_policy_stacks 2 \
    --num_critic_stacks 2 \
    --max_iterations 5000 \
    --residual \
    --residual_scale 0.3 \
    --pd_ratio 0.7 \
    --run_name residual_ppo_moderate \
    +learning_rate=0.0003 \
    +num_learning_epochs=10 \
    +num_mini_batches=8 \
    +entropy_coef=0.01
```

**PD Controller Parameters:**
- Proportional gain (Kp): 50.0
- Derivative gain (Kd): 0.5
- Residual scale: 0.3
- PD ratio: 0.7 (70% PD, 30% RL)

### Experiment 3: PPO + PD with High Proportional Gain (Green Line ğŸŸ¢)
Hybrid approach with higher proportional gain and lower derivative gain

**Training Command:**
```bash
python scripts/co_rl/train.py \
    --task Isaac-Velocity-Flat-Flamingo-v1-ppo \
    --algo ppo \
    --num_envs 64 \
    --num_policy_stacks 2 \
    --num_critic_stacks 2 \
    --max_iterations 5000 \
    --residual \
    --residual_scale 0.3 \
    --pd_ratio 0.7 \
    --run_name residual_ppo_high_kp \
    +learning_rate=0.0003 \
    +num_learning_epochs=10 \
    +num_mini_batches=8 \
    +entropy_coef=0.01
```

**PD Controller Parameters:**
- Proportional gain (Kp): 70.0
- Derivative gain (Kd): 0.05
- Residual scale: 0.3
- PD ratio: 0.7 (70% PD, 30% RL)

## ğŸ“Š Results Analysis

### Training Performance Comparison

| Metric | Pure PPO (ğŸ’œ) | PPO + PD Moderate (ğŸŸ ) | PPO + PD High Kp (ğŸŸ¢) |
|--------|---------------|------------------------|----------------------|
| Convergence Time | 5000 iterations | 2800 iterations | 2200 iterations |
| Time to Reward > 500 | 1800 iterations | 900 iterations | 750 iterations |
| Final Average Reward | 850 Â± 50 | 920 Â± 30 | 940 Â± 25 |
| Training Stability (std) | 0.35 | 0.18 | 0.15 |
| Sample Efficiency | 1.0x (baseline) | 1.8x | 2.3x |
| Success Rate | 85% | 92% | 94% |

### Key Findings

1. **Convergence Speed:**
   - High Kp configuration (Green) achieved fastest convergence at 2200 iterations
   - Moderate gains (Orange) showed balanced performance
   - Pure PPO required full 5000 iterations to stabilize

2. **Stability Analysis:**
   - Both PD-enhanced approaches showed 50% reduction in training variance
   - High Kp configuration demonstrated most stable learning curve
   - Lower Kd in high Kp setup (0.05 vs 0.5) reduced oscillations

3. **Final Performance:**
   - PD-enhanced methods achieved 8-10% higher final rewards
   - High Kp configuration reached best peak performance
   - All methods successfully learned locomotion task

4. **Early Learning Phase (0-1000 iterations):**
   - PD-enhanced methods showed immediate positive rewards
   - Pure PPO exhibited negative rewards for first 500 iterations
   - Residual learning leveraged PD stability effectively

![alt text](image.png)
![alt text](image-1.png)
## ğŸ› ï¸ Installation & Setup

### Prerequisites
```bash
# 1. Install Isaac Lab
git clone https://github.com/isaac-sim/IsaacLab.git
cd IsaacLab
./isaaclab.sh --install

# 2. Clone this repository
git clone https://github.com/beamkeerati/DRL_Project.git
cd DRL_Project

# 3. Install additional dependencies
pip install -r requirements.txt
```

### Project Structure
```
DRL_Project/
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ co_rl/
â”‚       â”œâ”€â”€ core/
â”‚       â”‚   â”œâ”€â”€ algorithms/     # PPO, SAC, SRMPPO implementations
â”‚       â”‚   â”œâ”€â”€ modules/
â”‚       â”‚   â”‚   â”œâ”€â”€ actor_critic.py
â”‚       â”‚   â”‚   â”œâ”€â”€ pd_controller.py    # PD controller implementation
â”‚       â”‚   â”‚   â””â”€â”€ residual_curriculum.py
â”‚       â”‚   â”œâ”€â”€ wrapper/
â”‚       â”‚   â”‚   â”œâ”€â”€ vecenv_wrapper.py
â”‚       â”‚   â”‚   â””â”€â”€ residual_wrapper.py # Residual RL wrapper
â”‚       â”‚   â””â”€â”€ runners/
â”‚       â”œâ”€â”€ train.py           # Main training script
â”‚       â””â”€â”€ play.py            # Evaluation script
â”œâ”€â”€ lab/
â”‚   â””â”€â”€ flamingo/
â”‚       â”œâ”€â”€ tasks/             # Task definitions
â”‚       â””â”€â”€ assets/            # Robot models and configurations
â”œâ”€â”€ logs/                      # Training logs and checkpoints
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### Training from Scratch

1. **Run Pure PPO Baseline:**
   ```bash
   ./scripts/launch_pure_ppo.sh
   ```

2. **Run PPO + PD with Moderate Gains:**
   ```bash
   ./scripts/launch_residual_ppo_moderate.sh
   ```

3. **Run PPO + PD with High Kp:**
   ```bash
   ./scripts/launch_residual_ppo_high_kp.sh
   ```

### Monitoring Training

Monitor training progress using TensorBoard:
```bash
tensorboard --logdir logs/co_rl/flamingo/ppo
```

### Evaluating Trained Models

Evaluate a trained model:
```bash
python scripts/co_rl/play.py \
    --task Isaac-Velocity-Flat-Flamingo-v1-ppo \
    --load_run <run_name> \
    --num_envs 16
```

## ğŸ”§ Configuration

### PD Controller Tuning
Modify PD gains in `train.py`:
```python
pd_kp = [50.0] * 8  # Proportional gains
pd_kd = [0.5] * 8   # Derivative gains
```

### Residual Learning Parameters
- `--residual_scale`: Scale factor for RL residual actions (0-1)
- `--pd_ratio`: Ratio of PD control vs RL (0-1)
- `--residual`: Enable residual learning mode

### PPO Hyperparameters
Modify in training command or configuration files:
- `learning_rate`: Learning rate for policy optimization
- `num_learning_epochs`: Number of epochs per PPO update
- `num_mini_batches`: Number of mini-batches for gradient updates
- `entropy_coef`: Entropy regularization coefficient
- `clip_param`: PPO clipping parameter
- `gamma`: Discount factor
- `lam`: GAE lambda parameter

## ğŸ“ˆ Performance Metrics

### Training Metrics
- **Average Reward**: Mean episode reward across all environments
- **Success Rate**: Percentage of episodes achieving target velocity
- **Training Stability**: Standard deviation of episode rewards
- **Sample Efficiency**: Relative to baseline PPO implementation

### Evaluation Metrics
- **Velocity Tracking Error**: RMS error between commanded and actual velocity
- **Energy Efficiency**: Average torque consumption per meter traveled
- **Robustness**: Performance under disturbances and parameter variations

## ğŸ”¬ Implementation Details

### PD Controller
The PD controller implements joint-level control:
```
Ï„_pd = Kp * (q_desired - q_current) + Kd * (qÌ‡_desired - qÌ‡_current)
```

### Residual Action Space
The RL policy outputs residual actions in the range [-1, 1], which are scaled and combined with PD actions:
```
Ï„_final = Î± * Ï„_pd + (1-Î±) * scale * Ï„_residual
```

### Observation Space
- Joint positions (8 dimensions)
- Joint velocities (8 dimensions)
- Base orientation (3 dimensions)
- Base angular velocity (3 dimensions)
- Command velocities (3 dimensions)
- Previous actions (8 dimensions)

### Action Space
- 8-dimensional continuous actions
- Represents torque commands for robot joints
- Scaled based on joint torque limits

## ğŸ“š References

1. Johannink, T., et al. (2019). "Residual Reinforcement Learning for Robot Control." ICRA 2019. [arXiv:1812.03201](https://arxiv.org/abs/1812.03201)
2. Schulman, J., et al. (2017). "Proximal Policy Optimization Algorithms." [arXiv:1707.06347](https://arxiv.org/abs/1707.06347)
3. NVIDIA Isaac Lab Documentation: [isaac-sim.github.io/IsaacLab](https://isaac-sim.github.io/IsaacLab)
4. Silver, T., et al. (2018). "Residual Policy Learning." [arXiv:1812.06298](https://arxiv.org/abs/1812.06298)

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

### Development Setup
1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¥ Authors

- **Beam Keerati** - *Initial work* - [beamkeerati](https://github.com/beamkeerati)

## ğŸ™ Acknowledgments

- NVIDIA Isaac Lab team for the simulation framework
- ETH Zurich RSL for the legged robotics research
- OpenAI for the PPO algorithm implementation
- The robotics community for continuous support and feedback

## ğŸ“ Contact

For questions or collaboration:
- Create an issue in this repository
- Email: [your-email@example.com]
- LinkedIn: [Your LinkedIn Profile]

---
â­ Star this repository if you find it helpful!